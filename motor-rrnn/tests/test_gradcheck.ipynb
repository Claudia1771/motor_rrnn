{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69191b94",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.network import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b65992",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def numeric_gradient(f, x, eps=1e-5):\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old = x[idx]\n",
    "        x[idx] = old + eps\n",
    "        fx1 = f()\n",
    "        x[idx] = old - eps\n",
    "        fx2 = f()\n",
    "        x[idx] = old\n",
    "        grad[idx] = (fx1 - fx2) / (2 * eps)\n",
    "        it.iternext()\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c850ec50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "net = NeuralNetwork(\n",
    "    input_dim=3,\n",
    "    layers_config=[\n",
    "        {\"units\": 4, \"activation\": \"sigmoid\"},\n",
    "        {\"units\": 2, \"activation\": \"softmax\"},\n",
    "    ],\n",
    "    loss=\"cross_entropy\"\n",
    ")\n",
    "\n",
    "X = np.random.randn(5, 3).astype(np.float32)\n",
    "y = np.array([0,1,0,1,1])\n",
    "y_oh = np.zeros((5,2)); y_oh[np.arange(5), y] = 1\n",
    "\n",
    "def f():\n",
    "    return net.compute_loss_and_gradients(X, y_oh)\n",
    "\n",
    "# forward + backward para tener gradientes anal√≠ticos\n",
    "loss = f()\n",
    "params_grads = net.get_params_and_grads()\n",
    "max_diff = 0.0\n",
    "for (p, g) in params_grads:\n",
    "    def f_param():\n",
    "        return net.compute_loss_and_gradients(X, y_oh)\n",
    "    num_g = numeric_gradient(f_param, p)\n",
    "    diff = np.max(np.abs(num_g - g))\n",
    "    max_diff = max(max_diff, diff)\n",
    "    print(\"max diff for param:\", diff)\n",
    "\n",
    "print(\"GLOBAL max diff:\", max_diff)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
