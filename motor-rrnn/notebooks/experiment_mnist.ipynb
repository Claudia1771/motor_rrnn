{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0b7d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils import load_mnist_from_npz, one_hot, get_batches\n",
    "from src.network import NeuralNetwork\n",
    "from src.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist_from_npz(\"data/mnist.npz\")\n",
    "num_classes = 10\n",
    "y_train_oh = one_hot(y_train, num_classes)\n",
    "y_test_oh = one_hot(y_test, num_classes)\n",
    "\n",
    "net = NeuralNetwork(\n",
    "    input_dim=784,\n",
    "    layers_config=[\n",
    "        {\"units\": 128, \"activation\": \"sigmoid\"},\n",
    "        {\"units\": num_classes, \"activation\": \"softmax\"},\n",
    "    ],\n",
    "    loss=\"cross_entropy\"\n",
    ")\n",
    "opt = Adam(lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "for ep in range(epochs):\n",
    "    for Xb, yb in get_batches(X_train, y_train_oh, batch_size, shuffle=True, seed=ep):\n",
    "        loss = net.compute_loss_and_gradients(Xb, yb)\n",
    "        opt.step(net.get_params_and_grads())\n",
    "    print(f\"Epoch {ep+1}/{epochs} - loss {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
